{
  "hash": "fab439633e38d383ebdb9aa92ef8f276",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: lecture 4 - multiple testing correction - with code\nauthor: \"Haky Im\"\ndate: \"March 31, 2025\"\ndescription: \"Lecture 4\"\ncategories:\n  - bios25328\n  - notebook\nfreeze: true\n---\n\n\n\nFind the lecture notes [here](https://www.icloud.com/keynote/07fVVMHEvj8f2aG5zyyFfXisA#L4-Multiple-Testing-Correction-2025).\n\n\n\n\n\n\n\n# Learning Objectives\n\n- Build intuition about p-values when multiple testing is performed via simulations.\n- Recognize the need for multiple testing correction.\n- Present methods to correct for multiple testing:\n  - Bonferroni correction\n  - FDR (false discovery rate)\n\n# Why Do We Need Multiple Testing Correction?\n\nWhen performing many statistical tests simultaneously, the probability of finding a \"significant\" result just by chance (a Type I error or false positive) increases substantially. This is famously illustrated by the XKCD comic on significance (link to XKCD #882). We need methods to control the overall error rate across all tests.\n\n# What Do P-values Look Like Under Null and Alternative Hypotheses?\n\n## Simulate Data: Null vs. Alternative\n\nWe'll start by simulating data under two scenarios:\n\n- Null Hypothesis (Ynull): The outcome variable is independent of the predictor X.\n- Alternative Hypothesis (Yalt): The outcome variable depends on the predictor X via the relationship Yalt = X * beta + epsilon.\n\n## Simulation Parameters\n\nFirst, define some parameters for the simulation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\nnsamp = 100     # Number of samples\nbeta = 2        # Effect size for the alternative hypothesis\nh2 = 0.1        # Proportion of variance in Yalt explained by X (related to signal strength)\nsig2X = h2      # Variance of X (scaled for simplicity here)\nsig2epsi = (1 - h2) * beta^2 # Variance of the error term epsilon, ensuring desired h2\nsigX = sqrt(sig2X)\nsigepsi = sqrt(sig2epsi)\n\nprint(paste(\"Variance of X:\", sig2X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Variance of X: 0.1\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Variance of Epsilon:\", sig2epsi))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Variance of Epsilon: 3.6\"\n```\n\n\n:::\n:::\n\n\n\n## Generate Single Instance of Data\n\nSimulate vectors X, epsilon (error), and Ynull (outcome under null). Then calculate Yalt.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate predictor X and error term epsi\nX = rnorm(nsamp, mean=0, sd= sigX)\nepsi = rnorm(nsamp, mean=0, sd=sigepsi)\n\n# Generate Ynull (independent of X, here just sampling with variance related to beta for comparison)\nYnull = rnorm(nsamp, mean=0, sd=beta) \n\n# Calculate Yalt = X * beta + epsi\nYalt = X * beta + epsi\n```\n:::\n\n\n\n## Visualize the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize Data\nplot(X, Yalt, main=\"Yalt vs X\"); grid()\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(X, Ynull, main=\"Ynull vs X\"); grid()\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n\n## Test Associations (Single Instance)\n\nTest the association between Ynull and X using linear regression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Ynull ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ynull ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3548 -1.1589  0.0782  1.3735  4.6291 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.0395     0.1951  -0.202    0.840\nX             1.0095     0.6240   1.618    0.109\n\nResidual standard error: 1.95 on 98 degrees of freedom\nMultiple R-squared:  0.02601,\tAdjusted R-squared:  0.01607 \nF-statistic: 2.617 on 1 and 98 DF,  p-value: 0.1089\n```\n\n\n:::\n:::\n\n\n\nQuestion: What is the p-value of the association between Ynull and X? Is it significant at the α=0.05 level?\n\nNow test the association between Yalt and X.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Yalt ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Yalt ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5453 -1.4809  0.1231  1.2189  4.1808 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.4244     0.1891  -2.244   0.0271 * \nX             2.0521     0.6050   3.392   0.0010 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.89 on 98 degrees of freedom\nMultiple R-squared:  0.1051,\tAdjusted R-squared:  0.09595 \nF-statistic: 11.51 on 1 and 98 DF,  p-value: 0.001001\n```\n\n\n:::\n:::\n\n\n\nQuestion: What is the p-value of the association between Yalt and X? Is it significant at the α=0.05 level?\n\n## Calculate the Empirical Distribution of P-values\n\nTo understand the distribution of p-values, we need to repeat the simulation many times.\n\n### Convenience Function fastlm\n\nWe'll run 10,000 regressions. A simplified function fastlm can speed this up compared to repeated calls to lm().\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfastlm = function(xx,yy) {\n  ## compute betahat (regression coef) and pvalue with Ftest\n  ## for now it does not take covariates\n  \n  df1 = 2 # DFs for model with predictor\n  df0 = 1 # DFs for null model (intercept only)\n  ind = !is.na(xx) & !is.na(yy)\n  xx = xx[ind]\n  yy = yy[ind]\n  n = sum(ind)\n  xbar = mean(xx)\n  ybar = mean(yy)\n  xx = xx - xbar # Center X\n  yy = yy - ybar # Center Y\n  \n  SXX = sum( xx^2 )\n  SYY = sum( yy^2 )\n  SXY = sum( xx * yy )\n  \n  betahat = SXY / SXX\n  \n  # RSS1 = Residual Sum of Squares for model Y ~ X\n  RSS1 = sum( ( yy - xx * betahat )^2 ) \n  # RSS0 = Residual Sum of Squares for model Y ~ 1 (equivalent to total sum of squares of centered Y)\n  RSS0 = SYY \n  \n  # F-statistic comparing the two models\n  fstat = ( ( RSS0 - RSS1 ) / ( df1 - df0 ) )  / ( RSS1 / ( n - df1 ) ) \n  # P-value from F-distribution\n  pval = 1 - pf(fstat, df1 = ( df1 - df0 ), df2 = ( n - df1 )) \n  \n  res = list(betahat = betahat, pval = pval)\n  return(res)\n}\n```\n:::\n\n\n\n### Run 10,000 Simulations\n\nSimulate X, Ynull, and Yalt 10,000 times. Store X and the outcomes in matrices.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim = 10000 # Number of simulations\n\n# Simulate matrices: rows=samples, cols=simulations\nXmat = matrix(rnorm(nsim * nsamp, mean=0, sd= sigX), nsamp, nsim)\nepsimat = matrix(rnorm(nsim * nsamp, mean=0, sd=sigepsi), nsamp, nsim)\n\n# Generate Y matrices based on the formulas\nYmat_alt = Xmat * beta + epsimat \nYmat_null = matrix(rnorm(nsim * nsamp, mean=0, sd=beta), nsamp, nsim) # Null Ys\n\n# Check dimensions\ndim(Ymat_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   100 10000\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(Ymat_alt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   100 10000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Assign column names for reference\ncolnames(Ymat_null) = paste0(\"sim\", 1:ncol(Ymat_null))\ncolnames(Ymat_alt) = colnames(Ymat_null)\n```\n:::\n\n\n\n### Calculate P-values Under the Null\n\nRun regressions for Ymat_null ~ Xmat for each simulation and store the p-values and coefficients.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npvec_null = rep(NA, nsim)\nbvec_null = rep(NA, nsim)\n\nfor(ss in 1:nsim) {\n  fit = fastlm(Xmat[,ss], Ymat_null[,ss])\n  pvec_null[ss] = fit$pval  \n  bvec_null[ss] = fit$betahat\n}\n\nsummary(pvec_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000021 0.2438476 0.4903663 0.4935828 0.7401156 0.9998408 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pvec_null, xlab=\"p-value\", main=\"Histogram of p-values under Null\", breaks=20)\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nQuestions:\n- The histogram should look approximately uniform. Why?\n- How many simulations under the null yield p-value below 0.05? What percentage is that?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(pvec_null < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 525\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pvec_null < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0525\n```\n\n\n:::\n:::\n\n\n\nWhat proportion of simulations do you expect to have p-values < α (for any α between 0 and 1) under the null?\nWhy does this uniform distribution highlight the need for correction when performing many tests?\n\n# Bonferroni Correction\n\nThe simplest correction is the Bonferroni correction.\n\nMethod: Divide the desired significance level (e.g., 0.05) by the total number of tests performed (m, here nsim). Use this adjusted value as the new significance threshold.\n\nGoal: Controls the Family-Wise Error Rate (FWER) - the probability of calling one or more significant results across all tests under the null. P(FP>=1) ≤ α.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBF_thres = 0.05 / nsim # Bonferroni significance threshold\nprint(paste(\"Bonferroni threshold:\", BF_thres))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Bonferroni threshold: 5e-06\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Number of Bonferroni significant associations under the NULL\nprint(paste(\"Found under Null:\", sum(pvec_null < BF_thres)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Found under Null: 1\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Proportion under Null:\", mean(pvec_null < BF_thres)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Proportion under Null: 1e-04\"\n```\n\n\n:::\n:::\n\n\n\nQuestions:\n- What is the Bonferroni threshold for significance in this simulation?\n- How many false positives (significant results under the null) did we find using this strict threshold?\n\n# Mix of Null and Alternative Hypotheses\n\nReal-world datasets (like GWAS) contain a mixture of true associations (alternative hypothesis) and null results. Let's simulate this.\n\n## Create Mixed Data\n\nAssume a certain proportion (prop_alt) of our simulations represent true associations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_alt = 0.20 # Define proportion of alternative Ys in the mixture\n\n# Create a selection vector: 1 if alternative, 0 if null\nselectvec = rbinom(nsim, 1, prop_alt) \nnames(selectvec) = colnames(Ymat_alt) # Keep track\ntable(selectvec) # Show counts of null (0) vs alternative (1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nselectvec\n   0    1 \n7996 2004 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create the mixed Y matrix\n# If selectvec[i] is 1, use Ymat_alt[,i]; if 0, use Ymat_null[,i]\nYmat_mix = sweep(Ymat_alt, 2, selectvec, FUN='*') + sweep(Ymat_null, 2, 1 - selectvec, FUN='*')\n```\n:::\n\n\n\n## Calculate P-values for Mixed Data\n\nRun regressions for Ymat_mix ~ Xmat.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npvec_mix = rep(NA, nsim)\nbvec_mix = rep(NA, nsim)\n\nfor(ss in 1:nsim) {\n  fit = fastlm(Xmat[,ss], Ymat_mix[,ss])\n  pvec_mix[ss] = fit$pval  \n  bvec_mix[ss] = fit$betahat\n}\n\nsummary(pvec_mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.07854 0.37313 0.40036 0.67907 0.99984 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pvec_mix, xlab=\"p-value\", main=\"Histogram of p-values under mixture\", breaks=20)\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## Interpreting Mixed P-values\n\nThe histogram shows a spike near zero (from true alternatives) superimposed on a uniform background (from true nulls).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthres = 0.05\nm_signif = sum(pvec_mix < thres)      # Observed number of significant associations\nm_expected_null = thres * sum(selectvec == 0) # Expected number of FPs from nulls\nm_expected_all_null = thres * nsim # Expected if *all* were null\n\nprint(paste(\"Observed significant (p<0.05):\", m_signif))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Observed significant (p<0.05): 2206\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Expected false positives if all were null:\", m_expected_all_null))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Expected false positives if all were null: 500\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Expected false positives from the actual nulls:\", round(m_expected_null)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Expected false positives from the actual nulls: 400\"\n```\n\n\n:::\n:::\n\n\n\nWe observed many more significant results than expected if all tests were null. How can we estimate the proportion of false discoveries among those we call significant?\n\n## False Discovery Rate (FDR) Estimate (using known truth)\n\nSince this is a simulation, we know which tests were truly null (selectvec == 0). We can calculate the actual FDR for a given p-value threshold.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# FDR = (Number of significant tests that are truly null) / (Total number of significant tests)\nFP = sum(pvec_mix < thres & selectvec == 0) # False Positives\nTP = sum(pvec_mix < thres & selectvec == 1) # True Positives\nS = sum(pvec_mix < thres) # Total Significant (S = FP + TP)\n\nFDR_sim = FP / S \nprint(paste(\"Simulated FDR at p <\", thres, \":\", round(FDR_sim, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Simulated FDR at p < 0.05 : 0.1886\"\n```\n\n\n:::\n:::\n\n\n\nThis means that if we used p < 0.05 as our criterion, about round(FDR_sim*100, 1)% of our significant findings would actually be false positives.\n\nQuestions:\n- What is the proportion of false discoveries (FDR) if we use a significance level of 0.01?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthres_01 = 0.01\nFP_01 = sum(pvec_mix < thres_01 & selectvec == 0)\nS_01 = sum(pvec_mix < thres_01)\nFDR_sim_01 = FP_01 / S_01\nprint(paste(\"Simulated FDR at p < 0.01:\", round(FDR_sim_01, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Simulated FDR at p < 0.01: 0.0582\"\n```\n\n\n:::\n:::\n\n\n\n- What is the proportion of false discoveries if we use the Bonferroni threshold?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFP_bf = sum(pvec_mix < BF_thres & selectvec == 0)\nS_bf = sum(pvec_mix < BF_thres)\nFDR_sim_bf = ifelse(S_bf > 0, FP_bf / S_bf, 0) # Avoid division by zero\nprint(paste(\"Simulated FDR at Bonferroni p <\", BF_thres, \":\", round(FDR_sim_bf, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Simulated FDR at Bonferroni p < 5e-06 : 0.0055\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Number significant at Bonferroni:\", S_bf))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Number significant at Bonferroni: 181\"\n```\n\n\n:::\n:::\n\n\n\n- What is the proportion of missed signals (False Negative Rate among true alternatives) using the Bonferroni threshold?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFN_bf = sum(pvec_mix >= BF_thres & selectvec == 1) # False Negatives\nTotal_Alt = sum(selectvec == 1) # Total true alternatives\nFNR_sim_bf = FN_bf / Total_Alt\nprint(paste(\"Simulated FNR at Bonferroni p <\", BF_thres, \":\", round(FNR_sim_bf, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Simulated FNR at Bonferroni p < 5e-06 : 0.9102\"\n```\n\n\n:::\n:::\n\n\n\n# Common Approaches to Control Type I Errors\n\nLet m be the total number of tests. Consider the following table:\n\n|| Called Not Significant | Called Significant | Total\n|---|---------------|---------------------|-----\n| Null True | TN | FP | $m_0$\n| Alternative True | FN | TP | $m_1$\n| Total | $m$ - S | S | $m$\n\n(m total # tests, $m_0$ # null tests, FP false positives, TP true positives, TN true negatives, FN false negatives, S total significant)\n\n- FWER (Family-Wise Error Rate): P(FP ≥ 1). Probability of at least one false positive. Controlled by Bonferroni. Often too strict.\n- FDR (False Discovery Rate): E[FP / S | S > 0]. Expected proportion of false positives among all significant results. A more liberal approach, often preferred when many tests are performed and finding true positives is important.\n\n## Table of Simulation Results\n\nLet's construct the confusion table for our mixed simulation using alpha = 0.05.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_table = t(table(Significant = pvec_mix < 0.05, True_Status = selectvec))\n# Reorder and rename for standard confusion matrix format\n# Rows: True Status (0=Null, 1=Alt); Columns: Called Status (FALSE=Not Sig, TRUE=Sig)\nconf_matrix = count_table[, c(\"FALSE\", \"TRUE\")] \ncolnames(conf_matrix) = c(\"Called Not Significant\", \"Called Significant\")\nrownames(conf_matrix) = c(\"Null True\", \"Alt True\")\n\nknitr::kable(conf_matrix, caption=\"Confusion Matrix at p < 0.05\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Confusion Matrix at p < 0.05\n\n|          | Called Not Significant| Called Significant|\n|:---------|----------------------:|------------------:|\n|Null True |                   7580|                416|\n|Alt True  |                    214|               1790|\n\n\n:::\n\n```{.r .cell-code}\n# Extracting values\nTN = conf_matrix[\"Null True\", \"Called Not Significant\"]\nFP = conf_matrix[\"Null True\", \"Called Significant\"]\nFN = conf_matrix[\"Alt True\", \"Called Not Significant\"]\nTP = conf_matrix[\"Alt True\", \"Called Significant\"]\nprint(paste(\"TN:\", TN, \" FP:\", FP, \" FN:\", FN, \" TP:\", TP))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"TN: 7580  FP: 416  FN: 214  TP: 1790\"\n```\n\n\n:::\n:::\n\n\n\n# Use qvalue Package to Estimate FDR\n\nIn real analysis, we don't know selectvec. The qvalue package estimates FDR and related quantities directly from the p-value distribution.\n\nq-value: The minimum FDR at which a test would be called significant. If you call all tests with q ≤ x significant, you expect the FDR to be approximately x.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install qvalue package if you haven't already\n# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n#BiocManager::install(\"qvalue\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(qvalue)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate q-values for the mixed p-values\nqres_mix = qvalue(pvec_mix)\nqvec_mix = qres_mix$qvalues\n\n# Also calculate for the null p-values for comparison\nqres_null = qvalue(pvec_null)\nqvec_null = qres_null$qvalues\n\nsummary(qvec_mix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000019 0.2391430 0.5681539 0.4635186 0.6894836 0.7615203 \n```\n\n\n:::\n:::\n\n\n\n## Visualize Q-values\n\nLet's see if small q-values correspond to true associations (selectvec = 1).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(qvec_mix ~ selectvec, main='Q-value by True Status', xlab=\"True Status (0=Null, 1=Alt)\", ylab=\"Q-value\")\ngrid()\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nPlot sorted q-values, colored by true status.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind = order(qvec_mix, decreasing=FALSE)\n# Using selectvec*2 + 1 maps 0->1 (black) and 1->3 (green) for plotting colors\nplot(sort(qvec_mix), col=selectvec[ind]*2 + 1, \n     pch=16, #selectvec[ind] + 1, \n     # lwd=selectvec[ind]*2 + 1,\n     main=\"Sorted Q-values by True Status\",\n     xlab=\"Tests ordered by Q-value\", ylab=\"Q-value\")\nlegend(\"bottomright\", legend=c(\"True Null\", \"True Alternative\"), pch=16, col=c(1, 3))\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\nQuestions:\n- Do small q-values tend to correspond to true alternatives (green points)?\n- Interpret the sorted q-value plot: What does the pattern show?\n\nCompare p-value and q-value distributions by true status.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nboxplot(pvec_mix ~ selectvec, main='P-value by True Status', xlab=\"True Status (0=Null, 1=Alt)\", ylab=\"P-value\"); grid()\nboxplot(qvec_mix ~ selectvec, main='Q-value by True Status', xlab=\"True Status (0=Null, 1=Alt)\", ylab=\"Q-value\"); grid()\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\n\nQuestion: Interpret these boxplots. How do q-values differ from p-values, especially for the null tests?\n\n## How Do Q-values and P-values Relate?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(pvec_null, qvec_null, main='Q-value vs P-value (All Null)', xlab=\"P-value\", ylab=\"Q-value\"); grid(); abline(0,1,col='red',lty=2)\nplot(pvec_mix, qvec_mix, main='Q-value vs P-value (Mixture)', xlab=\"P-value\", ylab=\"Q-value\"); grid(); abline(0,1,col='red',lty=2)\n```\n\n::: {.cell-output-display}\n![](index-with-code_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\n\nQ-values are monotonically increasing functions of p-values. Note that q-values are often ≥ p-values. The q-value can be interpreted as the FDR associated with calling that specific test significant.\n\nQuestions:\n- What is the smallest q-value when all simulations are from the null? Why?\n- What is the smallest q-value when simulations are from the mixture?\n\n## Estimating π₀ (Proportion of True Nulls)\n\nThe qvalue package also estimates π₀, the overall proportion of features (tests) that are truly null. \n\nπ₁ = 1 - π₀ is the proportion of true alternative hypotheses.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated pi0 when all tests were truly null\nprint(paste(\"Estimated pi0 (all null):\", round(qres_null$pi0, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated pi0 (all null): 0.9582\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimated pi0 for the mixed data\nprint(paste(\"Estimated pi0 (mixture):\", round(qres_mix$pi0, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated pi0 (mixture): 0.7616\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# True proportion used in simulation\nprint(paste(\"True pi0 used:\", 1 - prop_alt))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"True pi0 used: 0.8\"\n```\n\n\n:::\n:::\n\n\n\nQuestion: How well did the qvalue package estimate the true proportion of null hypotheses (pi0) in our mixed simulation?\n\n# References\n\nStorey, John D., and Robert Tibshirani. 2003. \"Statistical Significance for Genomewide Studies.\" Proceedings of the National Academy of Sciences 100 (16): 9440–45.",
    "supporting": [
      "index-with-code_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}